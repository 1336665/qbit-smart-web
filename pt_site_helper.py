#!/usr/bin/env python3
"""
é€šç”¨PTç«™ç‚¹è¾…åŠ©æ¨¡å— v1.0

æ”¯æŒæ‰€æœ‰NexusPHPæ¶æ„çš„PTç«™ç‚¹ï¼š
- é€šè¿‡ç§å­hashæœç´¢TID
- è·å–Peer Listä¸­çš„ç²¾ç¡®æ±‡æŠ¥æ—¶é—´
- ä¿ƒé”€ä¿¡æ¯æ£€æµ‹
- Cookieæœ‰æ•ˆæ€§æ£€æŸ¥

æ”¯æŒçš„ç«™ç‚¹æ¶æ„ï¼š
- NexusPHP (å¤§å¤šæ•°å›½å†…PTç«™)
- Gazelle (éƒ¨åˆ†å›½å¤–ç«™)
- Unit3D (æ–°æ¶æ„ç«™ç‚¹)

å·¥ä½œæµç¨‹ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. ä¼˜å…ˆä½¿ç”¨ç«™ç‚¹ç½‘é¡µè·å–æ±‡æŠ¥æ—¶é—´ï¼ˆæ›´ç²¾ç¡®ï¼‰                      â”‚
â”‚     â†“ å¤±è´¥åˆ™                                                 â”‚
â”‚  2. å›é€€åˆ°qBittorrent API                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
"""

import re
import time
import threading
import logging
from datetime import datetime
from functools import reduce
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, field
from urllib.parse import urlparse, urljoin
from enum import Enum

try:
    import requests
    REQUESTS_AVAILABLE = True
except ImportError:
    REQUESTS_AVAILABLE = False

try:
    from bs4 import BeautifulSoup
    BS4_AVAILABLE = True
except ImportError:
    BS4_AVAILABLE = False


class SiteType(Enum):
    """ç«™ç‚¹æ¶æ„ç±»å‹"""
    NEXUSPHP = "nexusphp"      # å¤§å¤šæ•°å›½å†…PTç«™
    GAZELLE = "gazelle"        # éƒ¨åˆ†å›½å¤–ç«™
    UNIT3D = "unit3d"          # æ–°æ¶æ„
    UNKNOWN = "unknown"


@dataclass
class TorrentSiteInfo:
    """ç§å­çš„ç«™ç‚¹ä¿¡æ¯"""
    torrent_hash: str
    site_id: int = 0
    site_name: str = ""
    tid: Optional[int] = None
    publish_time: Optional[float] = None
    promotion: str = "æœªçŸ¥"
    last_announce: Optional[float] = None
    uploaded_on_site: int = 0
    reannounce_in: Optional[int] = None  # è·ç¦»ä¸‹æ¬¡æ±‡æŠ¥çš„ç§’æ•°
    searched: bool = False
    search_time: float = 0
    error: str = ""
    source: str = ""  # æ•°æ®æ¥æº: "site" æˆ– "qb_api"


@dataclass
class PTSiteConfig:
    """PTç«™ç‚¹é…ç½®"""
    id: int
    name: str
    url: str
    cookie: str = ""
    tracker_keyword: str = ""
    enabled: bool = True
    site_type: SiteType = SiteType.NEXUSPHP
    
    # ç«™ç‚¹ç‰¹å®šé…ç½®
    search_path: str = "/torrents.php"
    search_param: str = "search"
    hash_search_area: str = "5"  # NexusPHP: 5=hashæœç´¢
    peerlist_path: str = "/viewpeerlist.php"
    
    # Cookieåç§°ï¼ˆä¸åŒç«™ç‚¹å¯èƒ½ä¸åŒï¼‰
    cookie_names: List[str] = field(default_factory=lambda: [
        "c_secure_uid", "c_secure_pass",  # å¸¸è§æ ¼å¼
        "nexusphp_u2",  # U2
        "PHPSESSID",    # é€šç”¨
    ])
    
    # æ±‡æŠ¥é—´éš”ä¼°ç®—ï¼ˆç§’ï¼‰
    announce_interval: int = 1800


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ç«™ç‚¹ç‰¹å®šé…ç½®é¢„è®¾
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
SITE_PRESETS: Dict[str, dict] = {
    # U2
    "u2.dmhy.org": {
        "site_type": SiteType.NEXUSPHP,
        "cookie_names": ["nexusphp_u2"],
        "announce_interval": 1800,
    },
    # é¦’å¤´
    "kp.m-team.cc": {
        "site_type": SiteType.NEXUSPHP,
        "cookie_names": ["c_secure_uid", "c_secure_pass"],
        "announce_interval": 1800,
    },
    "xp.m-team.io": {
        "site_type": SiteType.NEXUSPHP,
        "cookie_names": ["c_secure_uid", "c_secure_pass"],
        "announce_interval": 1800,
    },
    # çº¢å¶
    "leaves.red": {
        "site_type": SiteType.NEXUSPHP,
        "announce_interval": 1800,
    },
    # è§‚ä¼—
    "audiences.me": {
        "site_type": SiteType.NEXUSPHP,
        "announce_interval": 1800,
    },
    # HDSky
    "hdsky.me": {
        "site_type": SiteType.NEXUSPHP,
        "announce_interval": 1800,
    },
    # æœ±é›€
    "zhuque.in": {
        "site_type": SiteType.NEXUSPHP,
        "announce_interval": 1800,
    },
    # æµ·èƒ†
    "haidan.video": {
        "site_type": SiteType.NEXUSPHP,
        "announce_interval": 1800,
    },
    # çŒ«ç«™
    "pterclub.com": {
        "site_type": SiteType.NEXUSPHP,
        "announce_interval": 1800,
    },
    # æˆ‘å ¡
    "www.ourbits.club": {
        "site_type": SiteType.NEXUSPHP,
        "announce_interval": 1800,
    },
    # å†¬æ¨±
    "wintersakura.net": {
        "site_type": SiteType.NEXUSPHP,
        "announce_interval": 1800,
    },
    # å¹¼å„¿å›­
    "pt.ecust.pp.ua": {
        "site_type": SiteType.NEXUSPHP,
        "announce_interval": 1800,
    },
    # 1PTBA
    "1ptba.com": {
        "site_type": SiteType.NEXUSPHP,
        "announce_interval": 1800,
    },
    # è†éŸ³
    "pt.soulvoice.club": {
        "site_type": SiteType.NEXUSPHP,
        "announce_interval": 1800,
    },
    # éº’éºŸ
    "www.htpt.cc": {
        "site_type": SiteType.NEXUSPHP,
        "announce_interval": 1800,
    },
    # æŸ æª¬
    "leaguehd.com": {
        "site_type": SiteType.NEXUSPHP,
        "announce_interval": 1800,
    },
    # CHDBits
    "chdbits.co": {
        "site_type": SiteType.NEXUSPHP,
        "announce_interval": 1800,
    },
    "www.chdbits.co": {
        "site_type": SiteType.NEXUSPHP,
        "announce_interval": 1800,
    },
    # HDChina
    "hdchina.org": {
        "site_type": SiteType.NEXUSPHP,
        "announce_interval": 1800,
    },
    "www.hdchina.org": {
        "site_type": SiteType.NEXUSPHP,
        "announce_interval": 1800,
    },
    # TTG
    "totheglory.im": {
        "site_type": SiteType.NEXUSPHP,
        "announce_interval": 1800,
    },
    # KeepFRDS
    "keepfrds.com": {
        "site_type": SiteType.NEXUSPHP,
        "announce_interval": 1800,
    },
    "www.keepfrds.com": {
        "site_type": SiteType.NEXUSPHP,
        "announce_interval": 1800,
    },
    # PTHome
    "pthome.net": {
        "site_type": SiteType.NEXUSPHP,
        "announce_interval": 1800,
    },
    "www.pthome.net": {
        "site_type": SiteType.NEXUSPHP,
        "announce_interval": 1800,
    },
    # HDHome
    "hdhome.org": {
        "site_type": SiteType.NEXUSPHP,
        "announce_interval": 1800,
    },
    "www.hdhome.org": {
        "site_type": SiteType.NEXUSPHP,
        "announce_interval": 1800,
    },
    # LemonHDï¼ˆå¸¸è§åˆ«ååŸŸåï¼‰
    "lemonhd.org": {
        "site_type": SiteType.NEXUSPHP,
        "announce_interval": 1800,
    },
    "www.lemonhd.org": {
        "site_type": SiteType.NEXUSPHP,
        "announce_interval": 1800,
    },
    # BTSchool
    "pt.btschool.club": {
        "site_type": SiteType.NEXUSPHP,
        "announce_interval": 1800,
    },
    "btschool.club": {
        "site_type": SiteType.NEXUSPHP,
        "announce_interval": 1800,
    },
    # BYRPT
    "byr.pt": {
        "site_type": SiteType.NEXUSPHP,
        "announce_interval": 1800,
    },
    # Gazelleç«™ç‚¹ç¤ºä¾‹
    "passthepopcorn.me": {
        "site_type": SiteType.GAZELLE,
        "search_path": "/torrents.php",
        "peerlist_path": "/torrents.php",
        "announce_interval": 3600,
    },
}


class PTSiteHelper:
    """é€šç”¨PTç«™ç‚¹è¾…åŠ©ç±»"""
    
    VERSION = "1.0.0"
    
    # NexusPHPä¿ƒé”€å›¾æ ‡ç±»åæ˜ å°„
    PROMO_CLASSES = {
        'pro_free2up': ['Free', '2x'],
        'pro_free': ['Free'],
        'pro_2up': ['2x'],
        'pro_50pct': ['50%'],
        'pro_30pct': ['30%'],
        'pro_custom': ['Custom'],
        'free': ['Free'],
        'twoup': ['2x'],
        'twoupfree': ['Free', '2x'],
        'halfdown': ['50%'],
        'thirtypercent': ['30%'],
    }
    
    def __init__(self, site_config: PTSiteConfig, proxy: str = "", logger=None):
        """
        åˆå§‹åŒ–ç«™ç‚¹è¾…åŠ©å™¨
        
        Args:
            site_config: ç«™ç‚¹é…ç½®
            proxy: ä»£ç†åœ°å€ (å¯é€‰)
            logger: æ—¥å¿—è®°å½•å™¨
        """
        self.config = site_config
        self.proxy = proxy
        self.logger = logger or logging.getLogger(f"pt_helper_{site_config.name}")
        
        self._lock = threading.Lock()
        self._cookie_valid = False
        self._last_cookie_check = 0
        
        # åº”ç”¨ç«™ç‚¹é¢„è®¾
        self._apply_preset()
        
        # HTTPä¼šè¯
        self.session = None
        self.cookies = {}
        self.enabled = False
        
        if REQUESTS_AVAILABLE and site_config.cookie:
            self.session = requests.Session()
            self.session.headers['User-Agent'] = (
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
                '(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            )
            self.cookies = self._parse_cookie(site_config.cookie)
            self.user_id = self._extract_user_id()
            self.enabled = BS4_AVAILABLE and bool(self.cookies)
        
        # ç¼“å­˜
        self._tid_cache: Dict[str, TorrentSiteInfo] = {}
        self._cache_max_size = 1000
        self._user_id_checked = False
    
    def _apply_preset(self):
        """åº”ç”¨ç«™ç‚¹é¢„è®¾é…ç½®"""
        try:
            parsed = urlparse(self.config.url)
            domain = parsed.netloc.lower()
            
            # æŸ¥æ‰¾åŒ¹é…çš„é¢„è®¾
            for preset_domain, preset_config in SITE_PRESETS.items():
                if preset_domain in domain:
                    self.config.site_type = preset_config.get("site_type", SiteType.NEXUSPHP)
                    if "cookie_names" in preset_config:
                        self.config.cookie_names = preset_config["cookie_names"]
                    if "announce_interval" in preset_config:
                        self.config.announce_interval = preset_config["announce_interval"]
                    if "search_path" in preset_config:
                        self.config.search_path = preset_config["search_path"]
                    if "peerlist_path" in preset_config:
                        self.config.peerlist_path = preset_config["peerlist_path"]
                    self._log('debug', f"åº”ç”¨é¢„è®¾é…ç½®: {preset_domain}")
                    return
        except Exception as e:
            self._log('debug', f"åº”ç”¨é¢„è®¾å¤±è´¥: {e}")
    
    def _parse_cookie(self, cookie_str: str) -> dict:
        """
        è§£æCookieå­—ç¬¦ä¸²ï¼Œæ”¯æŒå¤šè¡Œæ ¼å¼
        
        æ”¯æŒçš„æ ¼å¼ï¼š
        1. å•è¡Œ: "name=value; name2=value2"
        2. å¤šè¡Œ: "name=value;\nname2=value2"
        3. çº¯valueï¼ˆå‡è®¾æ˜¯ä¸»è¦cookieï¼‰
        """
        cookies = {}
        if not cookie_str:
            return cookies
        
        # å…ˆæ¸…ç†Cookieæ ¼å¼
        import re
        # ç§»é™¤ä¸å¯è§å­—ç¬¦ï¼ˆBOMã€é›¶å®½å­—ç¬¦ç­‰ï¼‰
        cookie_str = re.sub(r'[\ufeff\ufffe\u200b\u200c\u200d\u2060\x00-\x1f\x7f-\x9f]', '', cookie_str)
        
        # å°†æ¢è¡Œç¬¦æ›¿æ¢ä¸ºåˆ†å·ï¼ˆæ”¯æŒå¤šè¡Œæ ¼å¼ï¼‰
        cookie_str = cookie_str.replace('\r\n', ';').replace('\r', ';').replace('\n', ';')
        
        # æ”¯æŒå¤šç§æ ¼å¼
        if '=' in cookie_str:
            for part in cookie_str.split(';'):
                part = part.strip()
                if '=' in part:
                    key, value = part.split('=', 1)
                    key = key.strip()
                    value = value.strip()
                    if key:  # ç¡®ä¿keyä¸ä¸ºç©º
                        cookies[key] = value
        else:
            # å°è¯•ä½¿ç”¨é¢„è®¾çš„cookieåç§°
            for name in self.config.cookie_names:
                cookies[name] = cookie_str.strip()
                break
        
        return cookies

    def _extract_user_id(self) -> Optional[int]:
        if not self.cookies:
            return None
        for key in ("c_secure_uid", "uid", "user_id", "userid"):
            value = self.cookies.get(key)
            if value and str(value).isdigit():
                return int(value)
        return None

    def _resolve_user_id(self) -> Optional[int]:
        if self._user_id_checked:
            return self.user_id
        self._user_id_checked = True
        if not self.enabled:
            return self.user_id
        try:
            base_url = self._get_base_url()
            html = self._request(f"{base_url}/index.php")
            if not html:
                return self.user_id
            match = re.search(r'userdetails\.php\?id=(\d+)', html)
            if match:
                self.user_id = int(match.group(1))
        except Exception:
            pass
        return self.user_id
    
    def _log(self, level: str, message: str):
        """è®°å½•æ—¥å¿—"""
        prefix = f"[{self.config.name}] "
        getattr(self.logger, level.lower(), self.logger.info)(prefix + message)
    
    def _get_base_url(self) -> str:
        """è·å–ç«™ç‚¹åŸºç¡€URL"""
        url = self.config.url.rstrip('/')
        if not url.startswith('http'):
            url = 'https://' + url
        return url

    def _get_site_host(self) -> str:
        """è·å–ç«™ç‚¹åŸŸå"""
        return urlparse(self._get_base_url()).netloc.lower()

    def _is_u2_site(self) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºU2ç«™ç‚¹"""
        return "u2.dmhy.org" in self._get_site_host()
    
    def _request(self, url: str, timeout: int = 15) -> Optional[str]:
        """å‘é€HTTPè¯·æ±‚"""
        if not self.session:
            self._log('warning', "Sessionæœªåˆå§‹åŒ–")
            return None
        
        self._log('debug', f"å‘èµ·è¯·æ±‚: {url} (è¶…æ—¶: {timeout}ç§’)")
        
        try:
            proxies = {'http': self.proxy, 'https': self.proxy} if self.proxy else None
            resp = self.session.get(
                url, 
                cookies=self.cookies, 
                proxies=proxies, 
                timeout=(10, timeout),  # (è¿æ¥è¶…æ—¶, è¯»å–è¶…æ—¶)
                allow_redirects=True
            )
            self._log('debug', f"è¯·æ±‚å®Œæˆ: HTTP {resp.status_code}")
            if resp.status_code == 200:
                return resp.text
            else:
                self._log('warning', f"è¯·æ±‚å¤±è´¥ HTTP {resp.status_code}: {url}")
        except requests.exceptions.Timeout as e:
            self._log('warning', f"è¯·æ±‚è¶…æ—¶ {url}: {e}")
        except requests.exceptions.ConnectionError as e:
            self._log('warning', f"è¿æ¥å¤±è´¥ {url}: {e}")
        except Exception as e:
            self._log('debug', f"è¯·æ±‚å¼‚å¸¸ {url}: {e}")
        return None
    
    def close(self):
        """å…³é—­ä¼šè¯"""
        if self.session:
            try:
                self.session.close()
            except:
                pass
    
    def update_cookie(self, cookie: str):
        """æ›´æ–°Cookie"""
        self.config.cookie = cookie
        self.cookies = self._parse_cookie(cookie) if cookie else {}
        self.user_id = self._extract_user_id()
        self._user_id_checked = False
        self.enabled = bool(self.cookies) and BS4_AVAILABLE and REQUESTS_AVAILABLE
        self._cookie_valid = False
        self._last_cookie_check = 0
        self._log('info', "Cookieå·²æ›´æ–°")

    @staticmethod
    def _parse_idle_seconds(text: str) -> Optional[int]:
        text = (text or '').strip()
        if not text:
            return None
        if text in {"åˆšåˆš", "just now", "now"}:
            return 0
        if re.match(r'^\d{1,2}:\d{2}(:\d{2})?$', text):
            parts = list(map(int, text.split(':')))
            return reduce(lambda a, b: a * 60 + b, parts)

        total = 0
        matched = False
        patterns = [
            (r'(\d+)\s*å¤©', 86400),
            (r'(\d+)\s*(å°æ—¶|æ—¶|h|hr|hrs)', 3600),
            (r'(\d+)\s*(åˆ†é’Ÿ|åˆ†|m|min|mins)', 60),
            (r'(\d+)\s*(ç§’|s|sec|secs)', 1),
        ]
        for pattern, multiplier in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                matched = True
                total += int(match.group(1)) * multiplier
        return total if matched else None

    @staticmethod
    def _parse_reannounce_seconds(text: str) -> Optional[int]:
        text = (text or '').strip()
        if not text:
            return None
        indicators = ['å', 'å‰©ä½™', 'next', 'left', 'remaining', 'reannounce']
        if not any(key in text.lower() for key in indicators):
            return None
        return PTSiteHelper._parse_idle_seconds(text)
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Cookieæ£€æµ‹
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def check_cookie_valid(self) -> Tuple[bool, str]:
        """
        æ£€æŸ¥Cookieæ˜¯å¦æœ‰æ•ˆ
        
        Returns:
            (æ˜¯å¦æœ‰æ•ˆ, çŠ¶æ€æ¶ˆæ¯)
        """
        if not self.enabled:
            if not REQUESTS_AVAILABLE:
                return False, "ç¼ºå°‘requestsåº“ï¼Œè¯·å®‰è£…: pip install requests"
            if not BS4_AVAILABLE:
                return False, "ç¼ºå°‘BeautifulSoupåº“ï¼Œè¯·å®‰è£…: pip install beautifulsoup4"
            if not self.config.cookie:
                return False, "æœªé…ç½®Cookie"
            return False, "ç«™ç‚¹è¾…åŠ©å™¨æœªå¯ç”¨"
        
        try:
            base_url = self._get_base_url()
            if not self.session:
                return False, "Sessionæœªåˆå§‹åŒ–"

            proxies = {'http': self.proxy, 'https': self.proxy} if self.proxy else None
            resp = self.session.get(
                f'{base_url}/index.php',
                cookies=self.cookies,
                proxies=proxies,
                timeout=(10, 15),
                allow_redirects=True,
            )

            if resp.status_code != 200:
                return False, f"è¯·æ±‚å¤±è´¥ HTTP {resp.status_code}"

            html = resp.text or ''

            if not html:
                if self.proxy:
                    return False, f"æ— æ³•è¿æ¥ç«™ç‚¹ï¼ˆä½¿ç”¨ä»£ç†: {self.proxy[:30]}...ï¼‰"
                return False, "æ— æ³•è¿æ¥ç«™ç‚¹ï¼Œå¯èƒ½éœ€è¦é…ç½®ä»£ç†"

            url_lower = resp.url.lower()
            html_lower = html.lower()

            login_page_indicators = [
                'login.php',
                'takelogin.php',
                'name="username"',
                'name="password"',
                'action="login.php"',
                'action="takelogin.php"',
                'form id="login',
                'forgotpass.php',
                'è¯·ç™»å½•',
            ]
            for indicator in login_page_indicators:
                if indicator in html_lower or indicator in url_lower:
                    self._cookie_valid = False
                    return False, "Cookieå·²å¤±æ•ˆï¼Œè¯·é‡æ–°ç™»å½•è·å–"

            # ç™»å½•çŠ¶æ€ç‰¹å¾ï¼ˆNexusPHPé€šç”¨ï¼‰
            login_indicators = [
                'logout.php',      # ç™»å‡ºé“¾æ¥
                'userdetails.php', # ç”¨æˆ·è¯¦æƒ…é“¾æ¥
                'usercp.php',      # æ§åˆ¶é¢æ¿é“¾æ¥
                'mybonus.php',     # é­”åŠ›å€¼é¡µé¢
                'invite.php',      # é‚€è¯·é¡µé¢
                'messages.php',    # æ¶ˆæ¯é¡µé¢
            ]
            for indicator in login_indicators:
                if indicator in html_lower:
                    self._cookie_valid = True
                    self._last_cookie_check = time.time()
                    if self.user_id is None:
                        self._resolve_user_id()
                    return True, "Cookieæœ‰æ•ˆ"

            # ä¸­æ–‡ç™»å½•çŠ¶æ€ç‰¹å¾
            chinese_indicators = ['ç™»å‡º', 'é€€å‡ºç™»å½•', 'ä¸ªäººä¿¡æ¯', 'æ§åˆ¶é¢æ¿', 'æˆ‘çš„é­”åŠ›', 'æˆ‘çš„é‚€è¯·', 'ç«™å†…ä¿¡']
            for indicator in chinese_indicators:
                if indicator in html:
                    self._cookie_valid = True
                    self._last_cookie_check = time.time()
                    if self.user_id is None:
                        self._resolve_user_id()
                    return True, "Cookieæœ‰æ•ˆ"

            self._cookie_valid = False
            return False, "Cookieæ— æ•ˆæˆ–å·²è¿‡æœŸï¼ˆæœªæ£€æµ‹åˆ°ç™»å½•çŠ¶æ€ï¼‰"

        except Exception as e:
            error_msg = str(e)[:50]
            return False, f"æ£€æŸ¥å¤±è´¥: {error_msg}"
    
    def is_cookie_valid(self) -> bool:
        """è¿”å›Cookieæ˜¯å¦æœ‰æ•ˆ"""
        return self._cookie_valid
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # TIDæœç´¢
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def search_tid_by_hash(self, torrent_hash: str) -> Optional[TorrentSiteInfo]:
        """
        é€šè¿‡ç§å­hashæœç´¢TIDå’Œä¿ƒé”€ä¿¡æ¯
        
        Args:
            torrent_hash: ç§å­çš„info_hash
            
        Returns:
            TorrentSiteInfo æˆ– None
        """
        if not self.enabled:
            return None
        
        # æ£€æŸ¥ç¼“å­˜
        cache_key = f"{self.config.id}:{torrent_hash.lower()}"
        if cache_key in self._tid_cache:
            cached = self._tid_cache[cache_key]
            # ç¼“å­˜1å°æ—¶
            if time.time() - cached.search_time < 3600:
                return cached
        
        info = TorrentSiteInfo(
            torrent_hash=torrent_hash,
            site_id=self.config.id,
            site_name=self.config.name
        )
        
        # æ ¹æ®ç«™ç‚¹ç±»å‹é€‰æ‹©è§£ææ–¹æ³•
        if self.config.site_type == SiteType.NEXUSPHP:
            info = self._search_nexusphp(torrent_hash, info)
        elif self.config.site_type == SiteType.GAZELLE:
            info = self._search_gazelle(torrent_hash, info)
        else:
            info = self._search_nexusphp(torrent_hash, info)  # é»˜è®¤å°è¯•NexusPHP
        
        # ç¼“å­˜ç»“æœ
        if info.searched:
            self._cache_result(cache_key, info)
        
        return info
    
    def _search_nexusphp(self, torrent_hash: str, info: TorrentSiteInfo) -> TorrentSiteInfo:
        """NexusPHPç«™ç‚¹æœç´¢"""
        try:
            base_url = self._get_base_url()
            search_url = (
                f'{base_url}{self.config.search_path}?'
                f'{self.config.search_param}={torrent_hash}&'
                f'search_area={self.config.hash_search_area}'
            )
            
            html = self._request(search_url)
            if not html:
                info.error = "è¯·æ±‚å¤±è´¥"
                return info
            
            with self._lock:
                soup = BeautifulSoup(html.replace('\n', ''), 'lxml')
                
                # æŸ¥æ‰¾ç§å­è¡¨æ ¼
                table = soup.select('table.torrents')
                if not table:
                    # å°è¯•å…¶ä»–é€‰æ‹©å™¨
                    table = soup.select('table#torrent_table, table.torrent_table')
                
                if not table or len(table[0].contents) <= 1:
                    info.error = "æœªæ‰¾åˆ°ç§å­"
                    info.searched = True
                    info.search_time = time.time()
                    return info
                
                # è·å–ç¬¬ä¸€ä¸ªæ•°æ®è¡Œ
                rows = table[0].find_all('tr')
                if len(rows) < 2:
                    info.error = "æœªæ‰¾åˆ°ç§å­"
                    info.searched = True
                    info.search_time = time.time()
                    return info
                
                row = rows[1]  # è·³è¿‡è¡¨å¤´
                tds = row.find_all('td')
                
                if len(tds) < 2:
                    info.error = "è§£æå¤±è´¥"
                    return info
                
                # è·å–TID
                try:
                    # æŸ¥æ‰¾åŒ…å«ç§å­è¯¦æƒ…é“¾æ¥çš„td
                    for td in tds[:3]:
                        links = td.find_all('a', href=True)
                        for a_tag in links:
                            href = a_tag.get('href', '')
                            # åŒ¹é… details.php?id=xxx æˆ– ?id=xxx
                            match = re.search(r'(?:details\.php\?)?id=(\d+)', href)
                            if match:
                                info.tid = int(match.group(1))
                                break
                        if info.tid:
                            break
                except Exception as e:
                    self._log('debug', f"è·å–TIDå¤±è´¥: {e}")
                
                # è·å–å‘å¸ƒæ—¶é—´
                try:
                    time_elem = row.find('time')
                    if time_elem:
                        time_str = time_elem.get('datetime') or time_elem.get('title')
                        if time_str:
                            dt = datetime.fromisoformat(time_str.replace('Z', '+00:00'))
                            info.publish_time = dt.timestamp()
                except Exception as e:
                    self._log('debug', f"è·å–å‘å¸ƒæ—¶é—´å¤±è´¥: {e}")
                
                # è·å–ä¿ƒé”€ä¿¡æ¯
                try:
                    promos = []
                    # æŸ¥æ‰¾ä¿ƒé”€å›¾æ ‡
                    imgs = row.find_all('img')
                    for img in imgs:
                        classes = img.get('class', [])
                        if isinstance(classes, list):
                            c_str = " ".join(classes)
                        else:
                            c_str = str(classes)
                        
                        for class_name, promo_types in self.PROMO_CLASSES.items():
                            if class_name in c_str.lower():
                                promos.extend(promo_types)
                        
                        # æ£€æŸ¥altå’Œtitleå±æ€§
                        alt = (img.get('alt') or '').lower()
                        title = (img.get('title') or '').lower()
                        for text in [alt, title]:
                            if 'free' in text:
                                promos.append('Free')
                            if '2x' in text or '2up' in text or 'double' in text:
                                promos.append('2x')
                            if '50%' in text or 'half' in text:
                                promos.append('50%')
                    
                    # æŸ¥æ‰¾ä¿ƒé”€æ–‡å­—
                    text_content = row.get_text().lower()
                    if 'free' in text_content and 'Free' not in promos:
                        promos.append('Free')
                    
                    if promos:
                        info.promotion = " + ".join(sorted(list(set(promos)), 
                                                          key=lambda x: len(x), reverse=True))
                    else:
                        info.promotion = "æ— ä¼˜æƒ "
                except Exception as e:
                    self._log('debug', f"è·å–ä¿ƒé”€ä¿¡æ¯å¤±è´¥: {e}")
                    info.promotion = "æœªçŸ¥"
                
                info.searched = True
                info.search_time = time.time()
                info.source = "site"
                
                if info.tid:
                    self._log('info', f"ğŸ” Hash {torrent_hash[:8]}... â†’ tid={info.tid} | ä¼˜æƒ : {info.promotion}")
                
                return info
                
        except Exception as e:
            self._log('error', f"æœç´¢TIDå¤±è´¥: {e}")
            info.error = str(e)
            return info
    
    def _search_gazelle(self, torrent_hash: str, info: TorrentSiteInfo) -> TorrentSiteInfo:
        """Gazelleç«™ç‚¹æœç´¢ï¼ˆç®€åŒ–å®ç°ï¼‰"""
        # Gazelleç«™ç‚¹çš„æœç´¢é€»è¾‘
        # å¤§å¤šæ•°Gazelleç«™ç‚¹ä¸æ”¯æŒç›´æ¥hashæœç´¢ï¼Œè¿™é‡Œåšç®€åŒ–å¤„ç†
        info.error = "Gazelleç«™ç‚¹æš‚ä¸æ”¯æŒhashæœç´¢"
        info.searched = True
        info.search_time = time.time()
        return info
    
    def _cache_result(self, key: str, info: TorrentSiteInfo):
        """ç¼“å­˜æœç´¢ç»“æœ"""
        if len(self._tid_cache) >= self._cache_max_size:
            oldest_key = min(self._tid_cache.keys(), 
                           key=lambda k: self._tid_cache[k].search_time)
            del self._tid_cache[oldest_key]
        
        self._tid_cache[key] = info
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Peer Listä¿¡æ¯ï¼ˆç²¾ç¡®æ±‡æŠ¥æ—¶é—´ï¼‰
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def get_peer_list_info(self, tid: int) -> Optional[Dict[str, Any]]:
        """
        è·å–Peer Listä¿¡æ¯ï¼ŒåŒ…å«ç²¾ç¡®çš„æ±‡æŠ¥æ—¶é—´
        
        Args:
            tid: ç§å­ID
            
        Returns:
            {
                'uploaded': int,        # ç«™ç‚¹è®°å½•çš„ä¸Šä¼ é‡
                'last_announce': float, # ä¸Šæ¬¡æ±‡æŠ¥æ—¶é—´æˆ³
                'idle_seconds': int,    # ç©ºé—²ç§’æ•°
                'reannounce_in': int,   # è·ç¦»ä¸‹æ¬¡æ±‡æŠ¥çš„ä¼°ç®—ç§’æ•°
            }
        """
        if not self.enabled or not tid or tid < 0:
            return None
        
        try:
            base_url = self._get_base_url()
            url = f'{base_url}{self.config.peerlist_path}?id={tid}'
            html = self._request(url)
            if not html:
                return None
            
            with self._lock:
                soup = BeautifulSoup(html.replace('\n', ' '), 'lxml')
                candidates = self._collect_peerlist_candidates(soup)

                if not candidates:
                    return None

                if self.user_id is None:
                    self._resolve_user_id()

                selected = None
                if self.user_id:
                    for candidate in candidates:
                        if candidate['user_id'] == self.user_id:
                            selected = candidate
                            break
                if not selected:
                    selected = candidates[0]

                announce_interval = self.config.announce_interval or 1800
                idle_seconds = selected['idle_seconds']
                reannounce_in = selected.get('reannounce_in')
                if reannounce_in is None:
                    reannounce_in = max(0, announce_interval - idle_seconds)
                result = {
                    'uploaded': selected.get('uploaded'),
                    'idle_seconds': idle_seconds,
                    'last_announce': time.time() - idle_seconds,
                    'reannounce_in': reannounce_in,
                }

                return result
                
        except Exception as e:
            self._log('debug', f"è·å–PeerListå¤±è´¥: {e}")
            return None

    def _collect_peerlist_candidates(self, soup: BeautifulSoup) -> List[Dict[str, Any]]:
        """è§£æpeer listå€™é€‰è¡Œ"""
        if self._is_u2_site():
            return self._parse_u2_peerlist_candidates(soup)
        return self._parse_nexus_peerlist_candidates(soup)

    def _parse_u2_peerlist_candidates(self, soup: BeautifulSoup) -> List[Dict[str, Any]]:
        """è§£æU2 peer listå€™é€‰è¡Œï¼ˆå›ºå®šåˆ—ä½ç½®ï¼‰"""
        candidates = []
        tables = soup.find_all('table')
        for table in tables or []:
            rows = table.find_all('tr')
            for tr in rows:
                if not tr.get('bgcolor'):
                    continue
                if tr.find('th'):
                    continue

                tds = tr.find_all('td')
                if len(tds) <= 10:
                    continue

                row_user_id = None
                try:
                    for a_tag in tr.find_all('a', href=True):
                        match = re.search(r'userdetails\.php\?id=(\d+)', a_tag.get('href', ''))
                        if match:
                            row_user_id = int(match.group(1))
                            break
                except Exception:
                    row_user_id = None

                uploaded = None
                try:
                    uploaded_str = tds[1].get_text(' ').strip()
                    if uploaded_str:
                        uploaded = self._parse_size(uploaded_str)
                except Exception:
                    uploaded = None

                idle_seconds = None
                try:
                    idle_text = tds[10].get_text(' ').strip()
                    idle_seconds = self._parse_idle_seconds(idle_text)
                except Exception as e:
                    self._log('debug', f"è§£æç©ºé—²æ—¶é—´å¤±è´¥: {e}")

                if idle_seconds is None:
                    continue

                candidates.append({
                    'user_id': row_user_id,
                    'uploaded': uploaded,
                    'idle_seconds': idle_seconds,
                    'reannounce_in': None,
                })
        return candidates

    def _parse_nexus_peerlist_candidates(self, soup: BeautifulSoup) -> List[Dict[str, Any]]:
        """è§£æNexusPHP peer listå€™é€‰è¡Œ"""
        candidates = []
        tables = soup.find_all('table')

        for table in tables or []:
            rows = table.find_all('tr')
            for tr in rows:
                # è·³è¿‡è¡¨å¤´
                if tr.find('th'):
                    continue

                tds = tr.find_all('td')
                if len(tds) < 2:
                    continue

                row_user_id = None
                try:
                    for a_tag in tr.find_all('a', href=True):
                        match = re.search(r'userdetails\.php\?id=(\d+)', a_tag.get('href', ''))
                        if match:
                            row_user_id = int(match.group(1))
                            break
                except Exception:
                    row_user_id = None

                uploaded = None
                try:
                    for td in tds[:6]:
                        text = td.get_text(' ').strip()
                        if re.match(r'[\d,.]+\s*(B|KB|KiB|MB|MiB|GB|GiB|TB|TiB)', text):
                            uploaded = self._parse_size(text)
                            break
                except Exception:
                    uploaded = None

                idle_seconds = None
                reannounce_override = None
                try:
                    for td in tds[-6:]:
                        text = td.get_text(' ').strip()
                        if reannounce_override is None:
                            reannounce_override = self._parse_reannounce_seconds(text)
                        idle_seconds = self._parse_idle_seconds(text)
                        if idle_seconds is not None:
                            break
                except Exception as e:
                    self._log('debug', f"è§£æç©ºé—²æ—¶é—´å¤±è´¥: {e}")

                if idle_seconds is None:
                    continue

                candidates.append({
                    'user_id': row_user_id,
                    'uploaded': uploaded,
                    'idle_seconds': idle_seconds,
                    'reannounce_in': reannounce_override,
                })
        return candidates
    
    @staticmethod
    def _parse_size(size_str: str) -> int:
        """è§£æå¤§å°å­—ç¬¦ä¸²"""
        try:
            parts = size_str.strip().split()
            if len(parts) != 2:
                # å°è¯•åˆ†ç¦»æ•°å­—å’Œå•ä½
                match = re.match(r'([\d,.]+)\s*(B|KB|KiB|MB|MiB|GB|GiB|TB|TiB|PB|PiB)', 
                               size_str.strip(), re.I)
                if match:
                    parts = [match.group(1), match.group(2)]
                else:
                    return 0
            
            num = float(parts[0].replace(',', '.'))
            unit = parts[1].upper()
            units = {
                'B': 0, 
                'KB': 1, 'KIB': 1, 
                'MB': 2, 'MIB': 2, 
                'GB': 3, 'GIB': 3, 
                'TB': 4, 'TIB': 4, 
                'PB': 5, 'PIB': 5
            }
            exp = units.get(unit, 0)
            return int(num * (1024 ** exp))
        except:
            return 0
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ç»¼åˆæŸ¥è¯¢
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def get_torrent_info(self, torrent_hash: str, include_peer_info: bool = True) -> TorrentSiteInfo:
        """
        è·å–ç§å­çš„å®Œæ•´ç«™ç‚¹ä¿¡æ¯
        
        Args:
            torrent_hash: ç§å­hash
            include_peer_info: æ˜¯å¦åŒ…å«peeråˆ—è¡¨ä¿¡æ¯
            
        Returns:
            TorrentSiteInfo
        """
        # å…ˆæœç´¢TID
        info = self.search_tid_by_hash(torrent_hash)
        
        if not info:
            info = TorrentSiteInfo(
                torrent_hash=torrent_hash,
                site_id=self.config.id,
                site_name=self.config.name,
                error="æœç´¢å¤±è´¥"
            )
            return info
        
        # å¦‚æœæ‰¾åˆ°TIDä¸”éœ€è¦peerä¿¡æ¯ï¼Œè·å–è¯¦ç»†ä¿¡æ¯
        if info.tid and include_peer_info:
            peer_info = self.get_peer_list_info(info.tid)
            if peer_info:
                info.uploaded_on_site = peer_info.get('uploaded', 0)
                info.last_announce = peer_info.get('last_announce')
                info.reannounce_in = peer_info.get('reannounce_in')
                info.source = "site"
        
        return info
    
    def get_reannounce_time(self, torrent_hash: str = None, tid: int = None) -> Optional[int]:
        """
        è·å–è·ç¦»ä¸‹æ¬¡æ±‡æŠ¥çš„ç§’æ•°
        
        Args:
            torrent_hash: ç§å­hash (äºŒé€‰ä¸€)
            tid: ç§å­ID (äºŒé€‰ä¸€)
            
        Returns:
            è·ç¦»ä¸‹æ¬¡æ±‡æŠ¥çš„ç§’æ•°ï¼Œæˆ–None
        """
        if not self.enabled:
            return None
        
        # å¦‚æœåªæœ‰hashï¼Œå…ˆæœç´¢TID
        if tid is None and torrent_hash:
            info = self.search_tid_by_hash(torrent_hash)
            if info and info.tid:
                tid = info.tid
        
        if not tid:
            return None
        
        peer_info = self.get_peer_list_info(tid)
        if peer_info:
            return peer_info.get('reannounce_in')
        
        return None
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # çŠ¶æ€å’Œç»Ÿè®¡
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def get_status(self) -> Dict[str, Any]:
        """è·å–è¾…åŠ©å™¨çŠ¶æ€"""
        return {
            'site_id': self.config.id,
            'site_name': self.config.name,
            'site_url': self.config.url,
            'site_type': self.config.site_type.value,
            'enabled': self.enabled,
            'cookie_valid': self._cookie_valid,
            'last_cookie_check': self._last_cookie_check,
            'cache_size': len(self._tid_cache),
            'has_cookie': bool(self.config.cookie),
            'has_bs4': BS4_AVAILABLE,
            'has_requests': REQUESTS_AVAILABLE,
            'announce_interval': self.config.announce_interval,
        }
    
    def clear_cache(self):
        """æ¸…é™¤TIDç¼“å­˜"""
        self._tid_cache.clear()
        self._log('info', "TIDç¼“å­˜å·²æ¸…é™¤")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ç«™ç‚¹è¾…åŠ©å™¨ç®¡ç†å™¨
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
class PTSiteHelperManager:
    """
    ç®¡ç†å¤šä¸ªç«™ç‚¹çš„è¾…åŠ©å™¨
    
    ä½¿ç”¨æ–¹æ³•ï¼š
    1. æ·»åŠ ç«™ç‚¹é…ç½®
    2. é€šè¿‡trackerå…³é”®å­—åŒ¹é…ç«™ç‚¹
    3. è·å–æ±‡æŠ¥æ—¶é—´ï¼ˆä¼˜å…ˆç«™ç‚¹ç½‘é¡µï¼Œå¤±è´¥åˆ™ç”¨qB APIï¼‰
    """
    
    def __init__(self, logger=None):
        self.logger = logger or logging.getLogger("pt_helper_manager")
        self._helpers: Dict[int, PTSiteHelper] = {}  # site_id -> helper
        self._tracker_map: Dict[str, int] = {}  # tracker_keyword -> site_id
        self._lock = threading.RLock()  # ä½¿ç”¨å¯é‡å…¥é”ï¼Œé¿å…åµŒå¥—è°ƒç”¨æ­»é”
    
    def add_site(self, site_config: PTSiteConfig, proxy: str = "") -> PTSiteHelper:
        """æ·»åŠ ç«™ç‚¹"""
        with self._lock:
            helper = PTSiteHelper(site_config, proxy, self.logger)
            self._helpers[site_config.id] = helper
            
            # å»ºç«‹trackerå…³é”®å­—æ˜ å°„
            if site_config.tracker_keyword:
                self._tracker_map[site_config.tracker_keyword.lower()] = site_config.id
            
            # è‡ªåŠ¨ä»URLæå–å…³é”®å­—
            try:
                parsed = urlparse(site_config.url)
                domain = parsed.netloc.lower()
                if domain:
                    self._tracker_map[domain] = site_config.id
                    # ä¹Ÿæ·»åŠ ä¸å¸¦wwwçš„ç‰ˆæœ¬
                    if domain.startswith('www.'):
                        self._tracker_map[domain[4:]] = site_config.id
            except:
                pass
            
            return helper
    
    def remove_site(self, site_id: int):
        """ç§»é™¤ç«™ç‚¹"""
        with self._lock:
            if site_id in self._helpers:
                self._helpers[site_id].close()
                del self._helpers[site_id]
            
            # æ¸…ç†trackeræ˜ å°„
            self._tracker_map = {k: v for k, v in self._tracker_map.items() if v != site_id}
    
    def get_helper(self, site_id: int) -> Optional[PTSiteHelper]:
        """è·å–ç«™ç‚¹è¾…åŠ©å™¨"""
        return self._helpers.get(site_id)
    
    def get_helper_by_tracker(self, tracker_url: str) -> Optional[PTSiteHelper]:
        """é€šè¿‡tracker URLè·å–å¯¹åº”çš„ç«™ç‚¹è¾…åŠ©å™¨"""
        if not tracker_url:
            return None
        
        tracker_lower = tracker_url.lower()
        
        for keyword, site_id in self._tracker_map.items():
            if keyword in tracker_lower:
                return self._helpers.get(site_id)
        
        return None
    
    def get_reannounce_time(self, torrent_hash: str, tracker_url: str, 
                           qb_reannounce: int = None) -> Tuple[Optional[int], str]:
        """
        è·å–æ±‡æŠ¥æ—¶é—´ï¼ˆå¸¦fallbackï¼‰
        
        Args:
            torrent_hash: ç§å­hash
            tracker_url: trackeråœ°å€
            qb_reannounce: qB APIè¿”å›çš„æ±‡æŠ¥æ—¶é—´ï¼ˆä½œä¸ºfallbackï¼‰
            
        Returns:
            (æ±‡æŠ¥å‰©ä½™ç§’æ•°, æ•°æ®æ¥æº)
            æ¥æº: "site" / "qb_api" / "unknown"
        """
        # 1. å°è¯•ä»ç«™ç‚¹è·å–
        helper = self.get_helper_by_tracker(tracker_url)
        if helper and helper.enabled:
            try:
                reannounce = helper.get_reannounce_time(torrent_hash=torrent_hash)
                if reannounce is not None:
                    return reannounce, "site"
            except Exception as e:
                self.logger.debug(f"ä»ç«™ç‚¹è·å–æ±‡æŠ¥æ—¶é—´å¤±è´¥: {e}")
        
        # 2. ä½¿ç”¨qB APIçš„å€¼
        if qb_reannounce is not None and qb_reannounce > 0:
            return qb_reannounce, "qb_api"
        
        return None, "unknown"
    
    def update_from_db(self, sites: List[dict], proxy: str = ""):
        """ä»æ•°æ®åº“é…ç½®æ›´æ–°ç«™ç‚¹"""
        with self._lock:
            # è·å–ç°æœ‰ç«™ç‚¹ID
            existing_ids = set(self._helpers.keys())
            new_ids = set()
            
            for site in sites:
                site_id = site['id']
                new_ids.add(site_id)
                
                if site_id in existing_ids:
                    # æ›´æ–°ç°æœ‰ç«™ç‚¹çš„cookie
                    helper = self._helpers[site_id]
                    if helper.config.cookie != site.get('cookie', ''):
                        helper.update_cookie(site.get('cookie', ''))
                else:
                    # æ·»åŠ æ–°ç«™ç‚¹
                    config = PTSiteConfig(
                        id=site_id,
                        name=site.get('name', ''),
                        url=site.get('url', ''),
                        cookie=site.get('cookie', ''),
                        tracker_keyword=site.get('tracker_keyword', ''),
                        enabled=site.get('enabled', True)
                    )
                    self.add_site(config, proxy)
            
            # ç§»é™¤å·²åˆ é™¤çš„ç«™ç‚¹
            for site_id in existing_ids - new_ids:
                self.remove_site(site_id)
    
    def get_all_status(self) -> List[Dict[str, Any]]:
        """è·å–æ‰€æœ‰ç«™ç‚¹çŠ¶æ€"""
        return [helper.get_status() for helper in self._helpers.values()]
    
    def close_all(self):
        """å…³é—­æ‰€æœ‰è¾…åŠ©å™¨"""
        for helper in self._helpers.values():
            helper.close()
        self._helpers.clear()
        self._tracker_map.clear()


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# å·¥å‚å‡½æ•°
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def create_site_helper(site_id: int, name: str, url: str, cookie: str = "", 
                       tracker_keyword: str = "", proxy: str = "") -> PTSiteHelper:
    """åˆ›å»ºç«™ç‚¹è¾…åŠ©å™¨å®ä¾‹"""
    config = PTSiteConfig(
        id=site_id,
        name=name,
        url=url,
        cookie=cookie,
        tracker_keyword=tracker_keyword
    )
    logger = logging.getLogger(f"pt_helper_{name}")
    return PTSiteHelper(config, proxy, logger)


def create_helper_manager() -> PTSiteHelperManager:
    """åˆ›å»ºç«™ç‚¹è¾…åŠ©å™¨ç®¡ç†å™¨"""
    logger = logging.getLogger("pt_helper_manager")
    return PTSiteHelperManager(logger)
