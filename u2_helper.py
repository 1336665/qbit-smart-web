#!/usr/bin/env python3
"""
U2ç½‘é¡µè¾…åŠ©æ¨¡å— v1.0

åŠŸèƒ½:
- Cookieæœ‰æ•ˆæ€§æ£€æµ‹
- TIDæœç´¢ï¼ˆé€šè¿‡ç§å­hashï¼‰
- ä¿ƒé”€ä¿¡æ¯æ£€æµ‹ï¼ˆFree/2x/50%ç­‰ï¼‰
- Peer Listä¿¡æ¯è·å–ï¼ˆç²¾ç¡®æ±‡æŠ¥æ—¶é—´ï¼‰
- å‘å¸ƒæ—¶é—´è·å–

ä¸ä¸»è„šæœ¬(main.py)åŠŸèƒ½ä¸€è‡´ï¼Œé€‚é…Webå¹³å°ä½¿ç”¨
"""

import re
import time
import threading
import logging
from datetime import datetime
from functools import reduce
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, field

try:
    import requests
    REQUESTS_AVAILABLE = True
except ImportError:
    REQUESTS_AVAILABLE = False

try:
    from bs4 import BeautifulSoup
    BS4_AVAILABLE = True
except ImportError:
    BS4_AVAILABLE = False


@dataclass
class TorrentU2Info:
    """ç§å­çš„U2ä¿¡æ¯"""
    torrent_hash: str
    tid: Optional[int] = None
    publish_time: Optional[float] = None
    promotion: str = "æœªçŸ¥"
    last_announce: Optional[float] = None
    uploaded_on_site: int = 0
    reannounce_in: Optional[int] = None  # è·ç¦»ä¸‹æ¬¡æ±‡æŠ¥çš„ç§’æ•°
    searched: bool = False
    search_time: float = 0
    error: str = ""


class U2WebHelper:
    """U2ç½‘é¡µè¾…åŠ©ç±»"""
    
    VERSION = "1.0.0"
    BASE_URL = "https://u2.dmhy.org"
    
    # ä¿ƒé”€å›¾æ ‡ç±»åæ˜ å°„
    PROMO_CLASSES = {
        'pro_free2up': ['Free', '2x'],
        'pro_free': ['Free'],
        'pro_2up': ['2x'],
        'pro_50pct': ['50%'],
        'pro_30pct': ['30%'],
        'pro_custom': ['Custom'],
    }
    
    def __init__(self, cookie: str = "", proxy: str = "", logger=None):
        """
        åˆå§‹åŒ–U2è¾…åŠ©å™¨
        
        Args:
            cookie: U2çš„nexusphp_u2 cookieå€¼
            proxy: ä»£ç†åœ°å€ (å¯é€‰)
            logger: æ—¥å¿—è®°å½•å™¨
        """
        self.cookie = cookie
        self.proxy = proxy
        self.logger = logger or logging.getLogger("u2_helper")
        
        self._lock = threading.Lock()
        self._cookie_valid = True
        self._last_cookie_check = 0
        
        # HTTPä¼šè¯
        self.session = None
        self.cookies = {}
        self.enabled = False
        
        if REQUESTS_AVAILABLE and cookie:
            self.session = requests.Session()
            self.session.headers['User-Agent'] = f'qBit-Smart-Web/U2Helper-{self.VERSION}'
            self.cookies = {'nexusphp_u2': cookie}
            self.enabled = BS4_AVAILABLE
        
        # ç¼“å­˜
        self._tid_cache: Dict[str, TorrentU2Info] = {}
        self._cache_max_size = 1000
    
    def _log(self, level: str, message: str):
        """è®°å½•æ—¥å¿—"""
        getattr(self.logger, level.lower(), self.logger.info)(message)
    
    def _request(self, url: str, timeout: int = 15) -> Optional[str]:
        """å‘é€HTTPè¯·æ±‚"""
        if not self.session:
            return None
        
        try:
            proxies = {'http': self.proxy, 'https': self.proxy} if self.proxy else None
            resp = self.session.get(url, cookies=self.cookies, proxies=proxies, timeout=timeout)
            if resp.status_code == 200:
                return resp.text
            else:
                self._log('warning', f"è¯·æ±‚å¤±è´¥ HTTP {resp.status_code}: {url}")
        except Exception as e:
            self._log('debug', f"è¯·æ±‚å¼‚å¸¸ {url}: {e}")
        return None
    
    def close(self):
        """å…³é—­ä¼šè¯"""
        if self.session:
            try:
                self.session.close()
            except:
                pass
    
    def update_cookie(self, cookie: str):
        """æ›´æ–°Cookie"""
        self.cookie = cookie
        self.cookies = {'nexusphp_u2': cookie} if cookie else {}
        self.enabled = bool(cookie) and BS4_AVAILABLE and REQUESTS_AVAILABLE
        self._cookie_valid = True
        self._log('info', "U2 Cookieå·²æ›´æ–°")
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Cookieæ£€æµ‹
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def check_cookie_valid(self) -> Tuple[bool, str]:
        """
        æ£€æŸ¥Cookieæ˜¯å¦æœ‰æ•ˆ
        
        Returns:
            (æ˜¯å¦æœ‰æ•ˆ, çŠ¶æ€æ¶ˆæ¯)
        """
        if not self.enabled:
            return False, "æœªé…ç½®Cookieæˆ–ç¼ºå°‘ä¾èµ–"
        
        try:
            html = self._request(f'{self.BASE_URL}/index.php', timeout=10)
            if not html:
                return False, "æ— æ³•è¿æ¥åˆ°U2"
            
            # æ£€æŸ¥ç™»å½•çŠ¶æ€ç‰¹å¾
            if 'logout.php' in html or 'ç™»å‡º' in html or 'userdetails.php' in html:
                self._cookie_valid = True
                self._last_cookie_check = time.time()
                return True, "Cookieæœ‰æ•ˆ"
            else:
                self._cookie_valid = False
                return False, "Cookieå·²å¤±æ•ˆï¼Œè¯·é‡æ–°ç™»å½•è·å–"
                
        except Exception as e:
            return False, f"æ£€æŸ¥å¤±è´¥: {e}"
    
    def is_cookie_valid(self) -> bool:
        """è¿”å›Cookieæ˜¯å¦æœ‰æ•ˆ"""
        return self._cookie_valid
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # TIDæœç´¢
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def search_tid_by_hash(self, torrent_hash: str) -> Optional[TorrentU2Info]:
        """
        é€šè¿‡ç§å­hashæœç´¢TIDå’Œä¿ƒé”€ä¿¡æ¯
        
        Args:
            torrent_hash: ç§å­çš„info_hash
            
        Returns:
            TorrentU2Info æˆ– None
        """
        if not self.enabled:
            return None
        
        # æ£€æŸ¥ç¼“å­˜
        cache_key = torrent_hash.lower()
        if cache_key in self._tid_cache:
            cached = self._tid_cache[cache_key]
            # ç¼“å­˜1å°æ—¶
            if time.time() - cached.search_time < 3600:
                return cached
        
        info = TorrentU2Info(torrent_hash=torrent_hash)
        
        try:
            url = f'{self.BASE_URL}/torrents.php?search={torrent_hash}&search_area=5'
            html = self._request(url)
            if not html:
                info.error = "è¯·æ±‚å¤±è´¥"
                return info
            
            with self._lock:
                soup = BeautifulSoup(html.replace('\n', ''), 'lxml')
                table = soup.select('table.torrents')
                
                if not table or len(table[0].contents) <= 1:
                    info.error = "æœªæ‰¾åˆ°ç§å­"
                    info.searched = True
                    info.search_time = time.time()
                    return info
                
                row = table[0].contents[1]
                if not hasattr(row, 'contents') or len(row.contents) < 2:
                    info.error = "è§£æå¤±è´¥"
                    return info
                
                # è·å–TID
                try:
                    link = row.contents[1]
                    href = ""
                    if hasattr(link, 'find'):
                        a_tag = link.find('a')
                        href = a_tag.get('href', '') if a_tag else ''
                    
                    match = re.search(r'id=(\d+)', href)
                    if match:
                        info.tid = int(match.group(1))
                except Exception as e:
                    self._log('debug', f"è·å–TIDå¤±è´¥: {e}")
                
                # è·å–å‘å¸ƒæ—¶é—´
                try:
                    if len(row.contents) > 3:
                        time_cell = row.contents[3]
                        if hasattr(time_cell, 'find'):
                            time_elem = time_cell.find('time')
                            if time_elem:
                                date_str = time_elem.get('title') or time_elem.get_text(' ')
                                if date_str:
                                    dt = datetime.strptime(date_str.strip(), '%Y-%m-%d %H:%M:%S')
                                    info.publish_time = dt.timestamp()
                except Exception as e:
                    self._log('debug', f"è·å–å‘å¸ƒæ—¶é—´å¤±è´¥: {e}")
                
                # è·å–ä¿ƒé”€ä¿¡æ¯
                try:
                    promos = []
                    imgs = row.contents[1].find_all('img')
                    for img in imgs:
                        classes = img.get('class', [])
                        if not classes:
                            continue
                        c_str = " ".join(classes) if isinstance(classes, list) else str(classes)
                        
                        for class_name, promo_types in self.PROMO_CLASSES.items():
                            if class_name in c_str:
                                promos.extend(promo_types)
                    
                    if promos:
                        info.promotion = " + ".join(sorted(list(set(promos)), key=lambda x: len(x), reverse=True))
                    else:
                        info.promotion = "æ— ä¼˜æƒ "
                except Exception as e:
                    self._log('debug', f"è·å–ä¿ƒé”€ä¿¡æ¯å¤±è´¥: {e}")
                    info.promotion = "æœªçŸ¥"
                
                info.searched = True
                info.search_time = time.time()
                
                # ç¼“å­˜ç»“æœ
                self._cache_result(cache_key, info)
                
                if info.tid:
                    self._log('info', f"ğŸ” Hash {torrent_hash[:8]}... â†’ tid={info.tid} | ä¼˜æƒ : {info.promotion}")
                
                return info
                
        except Exception as e:
            self._log('error', f"æœç´¢TIDå¤±è´¥: {e}")
            info.error = str(e)
            return info
    
    def _cache_result(self, key: str, info: TorrentU2Info):
        """ç¼“å­˜æœç´¢ç»“æœ"""
        # é™åˆ¶ç¼“å­˜å¤§å°
        if len(self._tid_cache) >= self._cache_max_size:
            # åˆ é™¤æœ€æ—§çš„æ¡ç›®
            oldest_key = min(self._tid_cache.keys(), 
                           key=lambda k: self._tid_cache[k].search_time)
            del self._tid_cache[oldest_key]
        
        self._tid_cache[key] = info
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Peer Listä¿¡æ¯ï¼ˆç²¾ç¡®æ±‡æŠ¥æ—¶é—´ï¼‰
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def get_peer_list_info(self, tid: int) -> Optional[Dict[str, Any]]:
        """
        è·å–Peer Listä¿¡æ¯ï¼ŒåŒ…å«ç²¾ç¡®çš„æ±‡æŠ¥æ—¶é—´
        
        Args:
            tid: ç§å­ID
            
        Returns:
            {
                'uploaded': int,        # ç«™ç‚¹è®°å½•çš„ä¸Šä¼ é‡
                'last_announce': float, # ä¸Šæ¬¡æ±‡æŠ¥æ—¶é—´æˆ³
                'idle_seconds': int,    # ç©ºé—²ç§’æ•°
                'reannounce_in': int,   # è·ç¦»ä¸‹æ¬¡æ±‡æŠ¥çš„ä¼°ç®—ç§’æ•°
            }
        """
        if not self.enabled or not tid or tid < 0:
            return None
        
        try:
            url = f'{self.BASE_URL}/viewpeerlist.php?id={tid}'
            html = self._request(url)
            if not html:
                return None
            
            with self._lock:
                soup = BeautifulSoup(html.replace('\n', ' '), 'lxml')
                tables = soup.find_all('table')
                result = {}
                
                for table in tables or []:
                    rows = table.find_all('tr')
                    for tr in rows:
                        # æŸ¥æ‰¾æœ‰èƒŒæ™¯è‰²çš„è¡Œï¼ˆæ•°æ®è¡Œï¼‰
                        if not tr.get('bgcolor'):
                            continue
                        
                        tds = tr.find_all('td')
                        if len(tds) < 2:
                            continue
                        
                        # è·å–ä¸Šä¼ é‡ (ç¬¬2åˆ—)
                        try:
                            uploaded_str = tds[1].get_text(' ').strip()
                            if uploaded_str:
                                result['uploaded'] = self._parse_size(uploaded_str)
                        except:
                            pass
                        
                        # è·å–ç©ºé—²æ—¶é—´ (ç¬¬11åˆ—ï¼Œæ ¼å¼ HH:MM:SS æˆ– MM:SS)
                        try:
                            if len(tds) > 10:
                                idle_str = tds[10].get_text(' ').strip()
                                if ':' in idle_str:
                                    parts = list(map(int, idle_str.split(':')))
                                    # è½¬æ¢ä¸ºç§’æ•°
                                    idle_seconds = reduce(lambda a, b: a * 60 + b, parts)
                                    result['idle_seconds'] = idle_seconds
                                    result['last_announce'] = time.time() - idle_seconds
                                    
                                    # ä¼°ç®—ä¸‹æ¬¡æ±‡æŠ¥æ—¶é—´
                                    # U2é»˜è®¤æ±‡æŠ¥é—´éš”çº¦1800ç§’ï¼Œæ ¹æ®ç§å­å¹´é¾„å¯èƒ½æ›´é•¿
                                    announce_interval = 1800  # åŸºç¡€é—´éš”30åˆ†é’Ÿ
                                    result['reannounce_in'] = max(0, announce_interval - idle_seconds)
                        except Exception as e:
                            self._log('debug', f"è§£æç©ºé—²æ—¶é—´å¤±è´¥: {e}")
                        
                        if result:
                            break
                    if result:
                        break
                
                return result if result else None
                
        except Exception as e:
            self._log('debug', f"è·å–PeerListå¤±è´¥: {e}")
            return None
    
    @staticmethod
    def _parse_size(size_str: str) -> int:
        """è§£æå¤§å°å­—ç¬¦ä¸²"""
        try:
            parts = size_str.strip().split()
            if len(parts) != 2:
                return 0
            num = float(parts[0].replace(',', '.'))
            unit = parts[1]
            units = {'B': 0, 'KiB': 1, 'MiB': 2, 'GiB': 3, 'TiB': 4, 'PiB': 5,
                    'KB': 1, 'MB': 2, 'GB': 3, 'TB': 4, 'PB': 5}
            exp = units.get(unit, 0)
            return int(num * (1024 ** exp))
        except:
            return 0
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ç»¼åˆæŸ¥è¯¢
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def get_torrent_info(self, torrent_hash: str, include_peer_info: bool = True) -> TorrentU2Info:
        """
        è·å–ç§å­çš„å®Œæ•´U2ä¿¡æ¯
        
        Args:
            torrent_hash: ç§å­hash
            include_peer_info: æ˜¯å¦åŒ…å«peeråˆ—è¡¨ä¿¡æ¯ï¼ˆéœ€è¦é¢å¤–è¯·æ±‚ï¼‰
            
        Returns:
            TorrentU2Info
        """
        # å…ˆæœç´¢TID
        info = self.search_tid_by_hash(torrent_hash)
        
        if not info:
            info = TorrentU2Info(torrent_hash=torrent_hash, error="æœç´¢å¤±è´¥")
            return info
        
        # å¦‚æœæ‰¾åˆ°TIDä¸”éœ€è¦peerä¿¡æ¯ï¼Œè·å–è¯¦ç»†ä¿¡æ¯
        if info.tid and include_peer_info:
            peer_info = self.get_peer_list_info(info.tid)
            if peer_info:
                info.uploaded_on_site = peer_info.get('uploaded', 0)
                info.last_announce = peer_info.get('last_announce')
                info.reannounce_in = peer_info.get('reannounce_in')
        
        return info
    
    def get_reannounce_time(self, torrent_hash: str = None, tid: int = None) -> Optional[int]:
        """
        è·å–è·ç¦»ä¸‹æ¬¡æ±‡æŠ¥çš„ç§’æ•°
        
        Args:
            torrent_hash: ç§å­hash (äºŒé€‰ä¸€)
            tid: ç§å­ID (äºŒé€‰ä¸€)
            
        Returns:
            è·ç¦»ä¸‹æ¬¡æ±‡æŠ¥çš„ç§’æ•°ï¼Œæˆ–None
        """
        if not self.enabled:
            return None
        
        # å¦‚æœåªæœ‰hashï¼Œå…ˆæœç´¢TID
        if tid is None and torrent_hash:
            info = self.search_tid_by_hash(torrent_hash)
            if info and info.tid:
                tid = info.tid
        
        if not tid:
            return None
        
        peer_info = self.get_peer_list_info(tid)
        if peer_info:
            return peer_info.get('reannounce_in')
        
        return None
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # çŠ¶æ€å’Œç»Ÿè®¡
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def get_status(self) -> Dict[str, Any]:
        """è·å–U2è¾…åŠ©å™¨çŠ¶æ€"""
        return {
            'enabled': self.enabled,
            'cookie_valid': self._cookie_valid,
            'last_cookie_check': self._last_cookie_check,
            'cache_size': len(self._tid_cache),
            'has_cookie': bool(self.cookie),
            'has_bs4': BS4_AVAILABLE,
            'has_requests': REQUESTS_AVAILABLE,
        }
    
    def clear_cache(self):
        """æ¸…é™¤TIDç¼“å­˜"""
        self._tid_cache.clear()
        self._log('info', "U2 TIDç¼“å­˜å·²æ¸…é™¤")


# å·¥å‚å‡½æ•°
def create_u2_helper(cookie: str = "", proxy: str = "") -> U2WebHelper:
    """åˆ›å»ºU2è¾…åŠ©å™¨å®ä¾‹"""
    logger = logging.getLogger("u2_helper")
    return U2WebHelper(cookie, proxy, logger)
